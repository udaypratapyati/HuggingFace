{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sharing pretrained models (TensorFlow)",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ir87Dnwno7Ia"
      },
      "source": [
        "# Sharing pretrained models (TensorFlow)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YXSTeboo7Ih"
      },
      "source": [
        "Install the Transformers and Datasets libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qo_3g9ibo7Ij"
      },
      "source": [
        "! pip install datasets transformers[sentencepiece]\n",
        "!pip install hf-lfs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhYvmWqto7Il"
      },
      "source": [
        "You will need to setup git, adapt your email and name in the following cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGWNL2MKo7Im"
      },
      "source": [
        "!git config --global user.email \"you@example.com\"\n",
        "!git config --global user.name \"Your Name\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZps31Nxo7Im"
      },
      "source": [
        "You will also need to be logged in to the Hugging Face Hub. Execute the following and enter your credentials."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBLrX1D1o7In"
      },
      "source": [
        "!huggingface-cli login"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70OFzi51o7Io"
      },
      "source": [
        "from transformers import TFAutoModelForMaskedLM, AutoTokenizer\n",
        "\n",
        "checkpoint = \"camembert-base\"\n",
        "\n",
        "model = TFAutoModelForMaskedLM.from_pretrained(checkpoint)\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dqrw3JB3o7Iq"
      },
      "source": [
        "model.push_to_hub(\"dummy-model\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDVatCZqo7Ir"
      },
      "source": [
        "tokenizer.push_to_hub(\"dummy-model\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIoNauBVo7Ir"
      },
      "source": [
        "tokenizer.push_to_hub(\"dummy-model\", organization=\"huggingface\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4cqfYF4o7Is"
      },
      "source": [
        "tokenizer.push_to_hub(\n",
        "    \"dummy-model\", organization=\"huggingface\", use_auth_token=\"<TOKEN>\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlU2ncwFo7It"
      },
      "source": [
        "from transformers import TFAutoModelForMaskedLM, AutoTokenizer\n",
        "\n",
        "checkpoint = \"camembert-base\"\n",
        "\n",
        "model = TFAutoModelForMaskedLM.from_pretrained(checkpoint)\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "# Do whatever with the model, train it, fine-tune it...\n",
        "\n",
        "model.save_pretrained(\"<path_to_dummy_folder>\")\n",
        "tokenizer.save_pretrained(\"<path_to_dummy_folder>\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}